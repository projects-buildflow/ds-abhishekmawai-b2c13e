name: Task Submission

on:
  pull_request:
    types: [opened, synchronize, reopened]

env:
  PYTHON_VERSION: "3.11"

jobs:
  # ============================================
  # Job 1: Detect task and validate
  # ============================================
  setup:
    name: Setup & Validate
    runs-on: ubuntu-latest
    outputs:
      task_id: ${{ steps.detect.outputs.task_id }}
      week: ${{ steps.detect.outputs.week }}
      has_notebook: ${{ steps.check-files.outputs.has_notebook }}
      has_screenshot: ${{ steps.check-files.outputs.has_screenshot }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect task from branch name
        id: detect
        run: |
          BRANCH="${{ github.head_ref }}"
          echo "Branch: $BRANCH"

          # Flexible task ID extraction - matches multiple patterns:
          # - task-1.2, task-1.2-description
          # - feature/1.2-something
          # - 1.2-fix-bug
          # - fix/task-1.2
          TASK_ID=$(echo "$BRANCH" | grep -oE '[0-9]+\.[0-9]+' | head -1 || echo "")

          # Get week number
          WEEK=$(echo "$TASK_ID" | cut -d'.' -f1)

          echo "task_id=$TASK_ID" >> $GITHUB_OUTPUT
          echo "week=$WEEK" >> $GITHUB_OUTPUT

          echo "Detected: Task=$TASK_ID, Week=$WEEK"

      - name: Validate branch naming
        run: |
          BRANCH="${{ github.head_ref }}"
          # Flexible validation - just needs to contain a task number pattern (X.Y)
          if [[ ! "$BRANCH" =~ [0-9]+\.[0-9]+ ]]; then
            echo "::error::Branch name must contain a task number (e.g., 1.2)"
            echo "Valid examples: task-1.2, task-1.2-fix-bug, feature/1.2, 1.2-my-work"
            exit 1
          fi

      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v41

      - name: Check for special files
        id: check-files
        run: |
          HAS_NOTEBOOK=false
          HAS_SCREENSHOT=false

          for file in ${{ steps.changed-files.outputs.all_changed_files }}; do
            if [[ "$file" == *.ipynb ]]; then
              HAS_NOTEBOOK=true
            fi
            if [[ "$file" == *.png ]] || [[ "$file" == *.jpg ]] || [[ "$file" == *.pdf ]]; then
              HAS_SCREENSHOT=true
            fi
          done

          echo "has_notebook=$HAS_NOTEBOOK" >> $GITHUB_OUTPUT
          echo "has_screenshot=$HAS_SCREENSHOT" >> $GITHUB_OUTPUT

  # ============================================
  # Job 2: Linting
  # ============================================
  lint:
    name: Lint Code
    runs-on: ubuntu-latest
    needs: setup

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install linters
        run: pip install ruff black

      - name: Run ruff
        id: ruff
        continue-on-error: true
        run: |
          ruff check . --output-format=github || echo "ruff_failed=true" >> $GITHUB_OUTPUT

      - name: Check formatting with black
        id: black
        continue-on-error: true
        run: |
          black --check . || echo "black_failed=true" >> $GITHUB_OUTPUT

      - name: Lint summary
        run: |
          if [[ "${{ steps.ruff.outputs.ruff_failed }}" == "true" ]] || [[ "${{ steps.black.outputs.black_failed }}" == "true" ]]; then
            echo "::warning::Linting issues found. Please fix them."
            echo ""
            echo "To fix formatting: black ."
            echo "To check issues: ruff check . --fix"
            exit 1
          fi
          echo "Linting passed"

  # ============================================
  # Job 3: Run Tests
  # ============================================
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    needs: setup
    outputs:
      passed: ${{ steps.test.outputs.passed }}
      test_score: ${{ steps.test.outputs.test_score }}
      output: ${{ steps.test.outputs.output }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install pytest pytest-json-report pandas pandera duckdb nbclient nbformat PyPDF2

      - name: Run task tests
        id: test
        run: |
          TASK_ID="${{ needs.setup.outputs.task_id }}"

          # Convert task ID to test file name (2.1 -> test_2_1.py)
          TEST_FILE="tests/test_${TASK_ID//./_}.py"

          echo "Looking for test file: $TEST_FILE"

          if [ ! -f "$TEST_FILE" ]; then
            echo "No test file found for task $TASK_ID"
            echo "passed=skip" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Run tests with JSON report, capture exit code
          set +e
          pytest "$TEST_FILE" -v \
            --json-report --json-report-file=test-report.json 2>&1 | tee test-output.txt
          TEST_EXIT_CODE=${PIPESTATUS[0]}
          set -e

          # Parse results
          if [ -f test-report.json ]; then
            PASSED=$(python -c "import json; r=json.load(open('test-report.json')); print(r['summary']['passed'])")
            FAILED=$(python -c "import json; r=json.load(open('test-report.json')); print(r['summary']['failed'])")
            TOTAL=$(python -c "import json; r=json.load(open('test-report.json')); print(r['summary']['total'])")

            echo "Results: $PASSED passed, $FAILED failed, $TOTAL total"

            # Enforce no-skip policy: skipped tests count as failures
            SKIPPED=$(python -c "import json; r=json.load(open('test-report.json')); print(r['summary'].get('skipped', 0))")

            # Compute test_score as percentage of passed tests
            TEST_SCORE=$(python -c "import json; r=json.load(open('test-report.json')); s=r['summary']; print(int(100*s['passed']/s['total']) if s['total']>0 else 0)")
            echo "test_score=$TEST_SCORE" >> $GITHUB_OUTPUT

            if [ "$FAILED" -eq "0" ] && [ "$PASSED" -gt "0" ] && [ "$SKIPPED" -eq "0" ]; then
              echo "passed=true" >> $GITHUB_OUTPUT
              echo "âœ… All tests passed! Score: $TEST_SCORE%"
            else
              echo "passed=false" >> $GITHUB_OUTPUT
              if [ "$SKIPPED" -gt "0" ]; then
                echo "âŒ $SKIPPED tests were skipped. All tests must pass without skips."
              else
                echo "âŒ Some tests failed. Please fix them before merging."
              fi
            fi
          else
            echo "No test report generated"
            echo "passed=false" >> $GITHUB_OUTPUT
            echo "test_score=0" >> $GITHUB_OUTPUT
          fi

          # Exit with the test result so the job properly fails
          exit $TEST_EXIT_CODE

      - name: Upload test report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-report
          path: |
            test-report.json
            test-output.txt
          if-no-files-found: ignore

  # ============================================
  # Job 4: AI Review (for notebooks/screenshots)
  # ============================================
  ai-review:
    name: AI Review
    runs-on: ubuntu-latest
    needs: [setup, test]
    if: needs.setup.outputs.has_notebook == 'true' || needs.setup.outputs.has_screenshot == 'true'

    steps:
      - name: Trigger AI review
        env:
          BACKEND_URL: ${{ secrets.BACKEND_URL }}
          WEBHOOK_SECRET: ${{ secrets.WEBHOOK_SECRET }}
        run: |
          if [ -z "$BACKEND_URL" ]; then
            echo "BACKEND_URL not configured, skipping AI review"
            exit 0
          fi

          curl -X POST "${BACKEND_URL}/webhooks/ai-review-complete" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${WEBHOOK_SECRET}" \
            -d '{
              "pr_number": ${{ github.event.pull_request.number }},
              "pr_url": "${{ github.event.pull_request.html_url }}",
              "task_id": "${{ needs.setup.outputs.task_id }}",
              "student_github": "${{ github.event.pull_request.user.login }}",
              "repo": "${{ github.repository }}",
              "has_notebook": ${{ needs.setup.outputs.has_notebook }},
              "has_screenshot": ${{ needs.setup.outputs.has_screenshot }}
            }' || echo "AI review request failed (non-fatal)"

          echo "AI review triggered"

  # ============================================
  # Job 5: Notify Backend
  # ============================================
  notify:
    name: Notify Backend
    runs-on: ubuntu-latest
    needs: [setup, lint, test]
    if: always()

    steps:
      - name: Send results to backend
        env:
          BACKEND_URL: ${{ secrets.BACKEND_URL }}
          WEBHOOK_SECRET: ${{ secrets.WEBHOOK_SECRET }}
        run: |
          if [ -z "$BACKEND_URL" ]; then
            echo "BACKEND_URL not configured, skipping notification"
            exit 0
          fi

          # Determine overall status
          LINT_PASSED="${{ needs.lint.result == 'success' }}"
          TEST_PASSED="${{ needs.test.outputs.passed }}"
          TEST_SCORE="${{ needs.test.outputs.test_score }}"

          # Convert skip to false â€” skipped tests should not count as passing
          if [ "$TEST_PASSED" = "skip" ]; then
            TEST_PASSED="false"
            TEST_SCORE="0"
          fi

          curl -X POST "${BACKEND_URL}/webhooks/github" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${WEBHOOK_SECRET}" \
            -d '{
              "event": "pr_check_complete",
              "pr_number": ${{ github.event.pull_request.number }},
              "pr_url": "${{ github.event.pull_request.html_url }}",
              "task_id": "${{ needs.setup.outputs.task_id }}",
              "student_github": "${{ github.event.pull_request.user.login }}",
              "branch_name": "${{ github.head_ref }}",
              "lint_passed": '"$LINT_PASSED"',
              "test_passed": "'"$TEST_PASSED"'",
              "test_score": '"$TEST_SCORE"',
              "repo": "${{ github.repository }}",
              "track": "ds"
            }' || echo "Backend notification failed (non-fatal)"

          echo "Backend notified"

  # ============================================
  # Job 6: Auto-merge PR when all checks pass
  # ============================================
  auto-merge:
    name: Auto Merge
    runs-on: ubuntu-latest
    needs: [setup, lint, test]
    if: |
      needs.lint.result == 'success' &&
      needs.test.outputs.passed == 'true'
    permissions:
      contents: write
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Approve and merge PR
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER=${{ github.event.pull_request.number }}
          TASK_ID=${{ needs.setup.outputs.task_id }}

          echo "âœ… All checks passed for Task ${TASK_ID}!"
          echo ""

          # Approve the PR
          echo "Approving PR #${PR_NUMBER}..."
          gh pr review ${PR_NUMBER} --approve --body "âœ… All checks passed! Auto-approved by BuildFlow."

          # Merge the PR with squash
          echo "Merging PR #${PR_NUMBER}..."
          gh pr merge ${PR_NUMBER} \
            --squash \
            --delete-branch \
            --body "ðŸŽ‰ Task ${TASK_ID} completed! Auto-merged by BuildFlow."

          echo ""
          echo "ðŸŽ‰ PR merged successfully!"
